{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5522e8",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Data Collection is the process of gathering raw information from different sources so it can later be analyzed.\n",
    "\n",
    "In Data Science and Machine Learning, models do not work on logic alone — they work on data.\n",
    "\n",
    "So the first step of any AI system is:\n",
    "\n",
    "Collect → Clean → Analyze → Model\n",
    "\n",
    "This notebook demonstrates how real-world data is collected using:\n",
    "- APIs\n",
    "- Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7d0e3",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "Data mainly exists in two forms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1f845",
   "metadata": {},
   "source": [
    "### Structured Data\n",
    "Organized and machine-friendly.\n",
    "\n",
    "Examples:\n",
    "- Tables (rows & columns)\n",
    "- CSV files\n",
    "- Databases\n",
    "- JSON APIs\n",
    "\n",
    "Easy to analyze using Pandas / SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770a56e",
   "metadata": {},
   "source": [
    "### Unstructured Data\n",
    "Not organized in fixed format.\n",
    "\n",
    "Examples:\n",
    "- Webpages (HTML)\n",
    "- Text documents\n",
    "- Images\n",
    "- Social media posts\n",
    "\n",
    "Requires extraction before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ab7a1",
   "metadata": {},
   "source": [
    "## APIs vs Web Scraping\n",
    "\n",
    "### API (Application Programming Interface)\n",
    "A server directly provides structured data.\n",
    "\n",
    "Advantages:\n",
    "- Clean data\n",
    "- Reliable\n",
    "- Fast\n",
    "- No parsing needed\n",
    "\n",
    "We simply request data and receive JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### Web Scraping\n",
    "Data is extracted from webpages (HTML).\n",
    "\n",
    "Advantages:\n",
    "- Works when API not available\n",
    "\n",
    "Disadvantages:\n",
    "- Requires parsing\n",
    "- Website structure may change\n",
    "- Slower than API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c573ff",
   "metadata": {},
   "source": [
    "In this notebook we will collect data using both methods\n",
    "and understand their differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bcb56",
   "metadata": {},
   "source": [
    "# Collecting Data using API\n",
    "\n",
    "An API returns structured data directly from the server.\n",
    "\n",
    "We send an HTTP request and receive JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69c360",
   "metadata": {},
   "source": [
    "Step 1 — Send Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e180416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://stephen-king-api.onrender.com/api/books\"\n",
    "\n",
    "response = requests.get(API)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f484d0",
   "metadata": {},
   "source": [
    "Step 2 — Inspect Response Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea9e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"id\":1,\"Year\":1974,\"Title\":\"Carrie\",\"handle\":\"carrie\",\"Publisher\":\"Doubleday\",\"ISBN\":\"978-0-385-08695-0\",\"Pages\":199,\"Notes\":[\"\"],\"created_at\":\"2023-11-13T23:48:47.848Z\",\"villains\":[{\"name\":\"Tina Blake\",\"url\":\"https://stephen-king-api.onrender.com/api/villain/4\"},{\"name\":\"Cindi\",\"url\":\"https://stephen-king-api.onrender.com/api/villain/14\"},{\"name\":\"Myra Crewes\",\"url\":\"https://stephen-king-api.onrender.com/api/villain/16\"},{\"name\":\"Billy deLois\",\"url\":\"https://stephen-king-api.onrender.\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f7c5b",
   "metadata": {},
   "source": [
    "Step 3 — Convert JSON to Python Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c04829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data = response.json()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c3adc",
   "metadata": {},
   "source": [
    "Preview first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bcade7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'Year': 1975,\n",
       " 'Title': \"Salem's Lot\",\n",
       " 'handle': 'salem-s-lot',\n",
       " 'Publisher': 'Doubleday',\n",
       " 'ISBN': '978-0-385-00751-1',\n",
       " 'Pages': 439,\n",
       " 'Notes': ['Nominee, World Fantasy Award, 1976[2]'],\n",
       " 'created_at': '2023-11-13T23:48:48.098Z',\n",
       " 'villains': [{'name': 'Kurt Barlow',\n",
       "   'url': 'https://stephen-king-api.onrender.com/api/villain/2'},\n",
       "  {'name': 'Richard Straker',\n",
       "   'url': 'https://stephen-king-api.onrender.com/api/villain/98'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"data\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aee785",
   "metadata": {},
   "source": [
    "Step 4 — Extract Useful Fields\n",
    "\n",
    "We only keep important columns instead of full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee2aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Carrie', 'year': 1974, 'publisher': 'Doubleday', 'pages': 199},\n",
       " {'title': \"Salem's Lot\",\n",
       "  'year': 1975,\n",
       "  'publisher': 'Doubleday',\n",
       "  'pages': 439},\n",
       " {'title': 'The Shining',\n",
       "  'year': 1977,\n",
       "  'publisher': 'Doubleday',\n",
       "  'pages': 447},\n",
       " {'title': 'Rage', 'year': 1977, 'publisher': 'Signet Books', 'pages': 211},\n",
       " {'title': 'The Stand', 'year': 1978, 'publisher': 'Doubleday', 'pages': 823}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for book in data[\"data\"]:\n",
    "    row = {\n",
    "        \"title\": book.get(\"Title\"),\n",
    "        \"year\": book.get(\"Year\"),\n",
    "        \"publisher\": book.get(\"Publisher\"),\n",
    "        \"pages\": book.get(\"Pages\")\n",
    "    }\n",
    "    dataset.append(row)\n",
    "\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a9513f",
   "metadata": {},
   "source": [
    "Step 5 — Clean Data<br>\n",
    "Convert numeric fields<br>\n",
    "Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9bf7f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Carrie', 'year': 1974, 'publisher': 'Doubleday', 'pages': 199},\n",
       " {'title': \"Salem's Lot\",\n",
       "  'year': 1975,\n",
       "  'publisher': 'Doubleday',\n",
       "  'pages': 439},\n",
       " {'title': 'The Shining',\n",
       "  'year': 1977,\n",
       "  'publisher': 'Doubleday',\n",
       "  'pages': 447},\n",
       " {'title': 'Rage', 'year': 1977, 'publisher': 'Signet Books', 'pages': 211},\n",
       " {'title': 'The Stand', 'year': 1978, 'publisher': 'Doubleday', 'pages': 823}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in dataset:\n",
    "    try:\n",
    "        row[\"year\"] = int(row[\"year\"])\n",
    "    except (TypeError, ValueError):\n",
    "        row[\"year\"] = None\n",
    "\n",
    "    try:\n",
    "        row[\"pages\"] = int(row[\"pages\"])\n",
    "    except (TypeError, ValueError):\n",
    "        row[\"pages\"] = None\n",
    "\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0921b",
   "metadata": {},
   "source": [
    "We now have structured dataset ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cd226",
   "metadata": {},
   "source": [
    "# Collecting Data using Web Scraping\n",
    "\n",
    "Unlike APIs, webpages return HTML instead of ready-to-use data.\n",
    "\n",
    "We must:\n",
    "1) Download page\n",
    "2) Parse HTML\n",
    "3) Extract useful information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b369cbb",
   "metadata": {},
   "source": [
    "Step 1 — Download Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b7ac96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "websiteURL = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(websiteURL, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf0aee",
   "metadata": {},
   "source": [
    "Step 2 — Parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9859a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298f272",
   "metadata": {},
   "source": [
    "Step 3 — Inspect Structure\n",
    "\n",
    "Each country is stored inside a block:\n",
    "\n",
    "<div class=\"country\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f98b28f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Countries: 250\n"
     ]
    }
   ],
   "source": [
    "countries = soup.find_all(\"div\", class_=\"country\")\n",
    "\n",
    "print(\"Total Countries:\", len(countries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3660b78",
   "metadata": {},
   "source": [
    "Preview first block to understand tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1deb8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"col-md-4 country\">\n",
      " <h3 class=\"country-name\">\n",
      "  <i class=\"flag-icon flag-icon-ad\">\n",
      "  </i>\n",
      "  Andorra\n",
      " </h3>\n",
      " <div class=\"country-info\">\n",
      "  <strong>\n",
      "   Capital:\n",
      "  </strong>\n",
      "  <span class=\"country-capital\">\n",
      "   Andorra la Vella\n",
      "  </span>\n",
      "  <br/>\n",
      "  <strong>\n",
      "   Population:\n",
      "  </strong>\n",
      "  <span class=\"country-population\">\n",
      "   84000\n",
      "  </span>\n",
      "  <br/>\n",
      "  <strong>\n",
      "   Area (km\n",
      "   <sup>\n",
      "    2\n",
      "   </sup>\n",
      "   ):\n",
      "  </strong>\n",
      "  <span class=\"country-area\">\n",
      "   468.0\n",
      "  </span>\n",
      "  <br/>\n",
      " </div>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(countries[0].prettify()[:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935baab",
   "metadata": {},
   "source": [
    "Step 4 — Extract Fields\n",
    "\n",
    "We collect:\n",
    "- name\n",
    "- capital\n",
    "- population\n",
    "- area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8892cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Andorra',\n",
       "  'capital': 'Andorra la Vella',\n",
       "  'population': '84000',\n",
       "  'area': '468.0'},\n",
       " {'name': 'United Arab Emirates',\n",
       "  'capital': 'Abu Dhabi',\n",
       "  'population': '4975593',\n",
       "  'area': '82880.0'},\n",
       " {'name': 'Afghanistan',\n",
       "  'capital': 'Kabul',\n",
       "  'population': '29121286',\n",
       "  'area': '647500.0'},\n",
       " {'name': 'Antigua and Barbuda',\n",
       "  'capital': \"St. John's\",\n",
       "  'population': '86754',\n",
       "  'area': '443.0'},\n",
       " {'name': 'Anguilla',\n",
       "  'capital': 'The Valley',\n",
       "  'population': '13254',\n",
       "  'area': '102.0'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []\n",
    "\n",
    "for c in countries:\n",
    "    name = c.find(\"h3\").text.strip()\n",
    "    capital = c.find(\"span\", class_=\"country-capital\").text.strip()\n",
    "    population = c.find(\"span\", class_=\"country-population\").text.strip()\n",
    "    area = c.find(\"span\", class_=\"country-area\").text.strip()\n",
    "\n",
    "    raw_data.append({\n",
    "        \"name\": name,\n",
    "        \"capital\": capital,\n",
    "        \"population\": population,\n",
    "        \"area\": area\n",
    "    })\n",
    "\n",
    "raw_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cc132",
   "metadata": {},
   "source": [
    "Step 5 — Clean Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c076616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Andorra',\n",
       "  'capital': 'Andorra la Vella',\n",
       "  'population': 84000,\n",
       "  'area': 468.0},\n",
       " {'name': 'United Arab Emirates',\n",
       "  'capital': 'Abu Dhabi',\n",
       "  'population': 4975593,\n",
       "  'area': 82880.0},\n",
       " {'name': 'Afghanistan',\n",
       "  'capital': 'Kabul',\n",
       "  'population': 29121286,\n",
       "  'area': 647500.0},\n",
       " {'name': 'Antigua and Barbuda',\n",
       "  'capital': \"St. John's\",\n",
       "  'population': 86754,\n",
       "  'area': 443.0},\n",
       " {'name': 'Anguilla',\n",
       "  'capital': 'The Valley',\n",
       "  'population': 13254,\n",
       "  'area': 102.0}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scraped = []\n",
    "\n",
    "for row in raw_data:\n",
    "    clean_row = {\n",
    "        \"name\": row[\"name\"],\n",
    "        \"capital\": row[\"capital\"],\n",
    "        \"population\": int(row[\"population\"].replace(\",\", \"\")),\n",
    "        \"area\": float(row[\"area\"])\n",
    "    }\n",
    "    dataset_scraped.append(clean_row)\n",
    "\n",
    "dataset_scraped[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dab631",
   "metadata": {},
   "source": [
    "Now the webpage data is converted into structured dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134a199",
   "metadata": {},
   "source": [
    "# API vs Web Scraping\n",
    "\n",
    "We collected data using two different techniques:\n",
    "\n",
    "API → structured JSON  \n",
    "Web Scraping → HTML extraction\n",
    "\n",
    "Both achieve the same goal but behave very differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3f104",
   "metadata": {},
   "source": [
    "## Key Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec72de",
   "metadata": {},
   "source": [
    "| Feature        | API                     | Web Scraping                     |\n",
    "|---------------|--------------------------|----------------------------------|\n",
    "| Data Format   | Structured (JSON)        | Unstructured (HTML)              |\n",
    "| Reliability   | High                     | Medium                           |\n",
    "| Speed         | Fast                     | Slower                           |\n",
    "| Maintenance   | Stable                   | Breaks when website changes      |\n",
    "| Legal Safety  | Allowed                  | Sometimes restricted             |\n",
    "| Complexity    | Easy                     | Moderate                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a47ba",
   "metadata": {},
   "source": [
    "## Reliability Comparison\n",
    "\n",
    "API:<br>\n",
    "Server is designed to provide data → predictable output\n",
    "\n",
    "Scraping:<br>\n",
    "Website designed for humans → structure may change\n",
    "\n",
    "So scraping code can stop working anytime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b9f1b",
   "metadata": {},
   "source": [
    "## When to Use API\n",
    "\n",
    "Use API when available:\n",
    "\n",
    "- official data access\n",
    "- stable format\n",
    "- faster processing\n",
    "- scalable pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429243a8",
   "metadata": {},
   "source": [
    "## When to Use Web Scraping\n",
    "\n",
    "Use scraping only when:\n",
    "\n",
    "- no API exists\n",
    "- data visible only on webpage\n",
    "- one-time dataset collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38305bd2",
   "metadata": {},
   "source": [
    "## Mental Rule\n",
    "\n",
    "Always try API first  \n",
    "Scrape only if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9581bf",
   "metadata": {},
   "source": [
    "# Saving Collected Data\n",
    "\n",
    "We store collected datasets so they can be used later in analysis.\n",
    "\n",
    "API data → JSON (structured hierarchical data)\n",
    "Scraped data → CSV (tabular data)\n",
    "\n",
    "To avoid cluttering the project, files will be saved inside a temporary folder created during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb6f29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files in: d:\\Workspace\\AI_ML\\ai-engineering-handbook\\06-Data-Collection\\output_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"../output_data\"\n",
    "\n",
    "# create folder only if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving files in:\", os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb229b3",
   "metadata": {},
   "source": [
    "## Save API Dataset as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc01584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API data saved: ../output_data\\books_api.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "api_file = os.path.join(output_dir, \"books_api.json\")\n",
    "\n",
    "with open(api_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n",
    "print(\"API data saved:\", api_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12acb",
   "metadata": {},
   "source": [
    "## Save Scraped Dataset as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d0a600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved: ../output_data\\countries_scraped.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = os.path.join(output_dir, \"countries_scraped.csv\")\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=dataset_scraped[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dataset_scraped)\n",
    "\n",
    "print(\"Scraped data saved:\", csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5dd61d",
   "metadata": {},
   "source": [
    "# Data Collection Pipeline Summary\n",
    "\n",
    "In this notebook we collected data using two different approaches:\n",
    "\n",
    "API → Structured JSON data  \n",
    "Web Scraping → Extracted HTML data\n",
    "\n",
    "Both were converted into structured datasets ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d64b9f",
   "metadata": {},
   "source": [
    "## Steps Followed\n",
    "\n",
    "1. Sent request to server\n",
    "2. Received response\n",
    "3. Parsed data\n",
    "4. Extracted useful fields\n",
    "5. Cleaned values\n",
    "6. Stored dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b64a41",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "\n",
    "API collection:\n",
    "Reliable and preferred method for data pipelines.\n",
    "\n",
    "Web scraping:\n",
    "Fallback technique when API not available.\n",
    "\n",
    "Both methods ultimately produce structured data suitable for Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504599c",
   "metadata": {},
   "source": [
    "## Real World Perspective\n",
    "\n",
    "In production systems:\n",
    "\n",
    "Databases → primary source  \n",
    "APIs → public/third-party data  \n",
    "Scraping → rare edge cases\n",
    "\n",
    "Most data engineers spend significant effort on data collection before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1041c",
   "metadata": {},
   "source": [
    "This notebook demonstrates the first step of any data science workflow:\n",
    "\n",
    "Collect → Clean → Analyze → Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
